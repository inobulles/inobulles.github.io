<!DOCTYPE html>
<html><head><meta charset=utf-8><title>netmap(4)</title><keywords content=man,netmap></keywords><style>:root { --monitor-bleed: rgba(183, 0, 255, 0.46); --gradient: -webkit-linear-gradient(50deg, rgb(126, 119, 255), rgb(255, 34, 137)); --background-colour: #fff; --title-colour: #000; --text-colour: #000; --article-colour: #fff; --accent-colour: #4385f5; --muted-colour: #555; --shadow-colour: rgba(183, 0, 255, 0.487); --border-radius: 12px; --blur-radius: 0px; --navbar-colour: #fffc; --navbar-shadow: rgba(183, 0, 255, 0.1) 0 8px 32px; --code-background: #ddd; } .dark { visibility: hidden; } @media (prefers-color-scheme: light) { } @media (prefers-color-scheme: dark) { :root { --monitor-bleed: rgba(183, 0, 255, 0.172); --gradient: -webkit-linear-gradient(50deg, rgb(255, 79, 79), rgb(255, 52, 221)); --background-colour: #000; --title-colour: #fff; --text-colour: #ddd; --article-colour: #00000033; --accent-colour: #98beff; --muted-colour: #999; --shadow-colour: #ff00cc29; --blur-radius: 96px; --navbar-colour: #000a; --navbar-shadow: rgba(183, 0, 255, 0.05) 0 8px 32px; --code-background: #111; } .dark { visibility: visible; } .light { visibility: hidden; } } html, body { overflow-x: hidden; } body { background-image: url(bg.svg); background-size: cover; margin: 0; background-color: var(--background-colour); color: var(--text-colour); font-size: 22px; font-display: optional; font-family: "Montserrat", sans-serif; overflow-y: hidden; } h1 { color: var(--title-colour); font-size: 70px; font-display: optional; font-family: "Montserrat", sans-serif; } h2 { color: var(--title-colour); font-size: 40px; font-display: optional; font-family: "Montserrat", sans-serif; } h3 { color: var(--title-colour); font-size: 24px; font-display: optional; font-family: "Roboto Slab", serif; } q, blockquote { font-style: italic; white-space: pre-wrap; padding-left: 42px; } pre { margin: 32px; white-space: pre-wrap; white-space: -moz-pre-wrap; white-space: -pre-wrap; white-space: -o-pre-wrap; word-wrap: break-word; } strong { background: var(--gradient); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent; font-weight: bold; } code { border-radius: 8px; background-color: var(--code-background); } pre { white-space: pre-wrap; } .info-container { max-width: 400px; margin: 32px; } .info-container p { margin-inline: 16px; margin-block: 32px; text-align: justify; font-size: 18px; } .labeled-img { max-width: 100%; position: relative; } .labeled-img div { position: absolute; bottom: 0; width: 100%; height: 70%; background: linear-gradient(to bottom, transparent, black); border-radius: var(--border-radius); } .labeled-img div h2 { color: white; position: absolute; bottom: 0; left: 32px; } .labeled-img img { width: 100%; border-radius: var(--border-radius); } a { background: var(--gradient); background-clip: none; color: var(--background-colour); font-weight: bold; text-decoration: none; border-radius: 3px; } a:hover { background: var(--text-colour); } .link, .link:hover { background: none; } button { background: var(--gradient); color: var(--background-colour); box-shadow: none; border: none; border-radius: 48px; padding: 8px; padding-left: 32px; padding-right: 32px; margin: 24px; font-size: 24px; font-display: optional; font-family: "Montserrat", sans-serif; font-weight: bold; cursor: pointer; transition: background .2s, color .2s, opacity .2s, box-shadow .2s, transform .2s; } button:hover { box-shadow: var(--shadow-colour) 0 8px 32px; transform: translateY(-4px); } button:active { transform: none; opacity: 50%; } .clickable { padding: 2px; transition: background-color 0.05s ease-out; border-radius: var(--border-radius); } .clickable:hover { background-color: var(--shadow-colour); } .docs-container { margin: auto; margin-top: 64px; max-width: 1200px; display: grid; justify-content: center; grid-template-columns: 1fr 3fr; } .docs { max-width: 900px; } .sidebar { min-width: 250px; position: fixed; padding: 16px; display: grid; justify-content: center; grid-auto-flow: row; grid-gap: 32px; padding-top: 64px; } .sidebar * { margin: auto; display: grid; justify-content: left; grid-auto-flow: column; grid-gap: 8px; } .sidebar p { font-size: 18px; } .sidebar-item { color: var(--text-colour); background: none; padding-inline: 8px; width: 100%; height: 100%; } .sidebar-item div svg { fill: var(--text-colour); } .navbar-container { width: 100vw; height: 48px; } .navbar { position: fixed; z-index: 100; width: 100vw; background-color: var(--navbar-colour); backdrop-filter: blur(20px); display: grid; justify-content: center; grid-auto-flow: column; grid-gap: 32px; padding: 12px; } .navbar * { margin: auto; display: grid; justify-content: center; grid-auto-flow: column; grid-gap: 8px; } .navbar p { font-size: 18px; } .navbar-logo { height: 100%; max-height: 32px; stroke: var(--text-colour); } .navbar-icon { fill: var(--text-colour); } .navbar-item { color: var(--text-colour); background-color: transparent; padding-inline: 8px; width: 100%; height: 100%; border-radius: 8px; } @media only screen and (max-width: 800px) { .navbar-item { position: absolute; visibility: hidden; } } footer { text-align: center; padding: 64px; border-radius: 64px 64px 0 0; background: -webkit-linear-gradient(50deg, transparent, var(--monitor-bleed)); } footer p { margin: auto; } .permalink { color: var(--text-colour); background: none; } .permalink:hover { background: none; } </style></head><body><html><body><div class=navbar-container><div class=navbar><a class="navbar-item clickable link" href=https://inobulles.github.io><div><svg class=navbar-logo fill=none height=24px viewbox="0 0 144 144" width=24px xmlns=http://www.w3.org/2000/svg><path d=m22,115l50,-88l50,88l-100,0z stroke-width=20 transform="rotate(180 72 72)"></path></svg><p><strong>aquaBSD</strong></p></div></a><a class="navbar-item clickable link" href=https://inobulles.github.io/docs><div><svg class=navbar-icon fill=none height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0V0z" fill=none></path><path d="M9.4 16.6L4.8 12l4.6-4.6L8 6l-6 6 6 6 1.4-1.4zm5.2 0l4.6-4.6-4.6-4.6L16 6l6 6-6 6-1.4-1.4z"></path></svg><p>Develop</p></div></a><a class="navbar-item clickable link" href=mailto:inobulles@gmail.com><div><svg class=navbar-icon fill=none height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0V0z" fill=none></path><path d="M6.54 5c.06.89.21 1.76.45 2.59l-1.2 1.2c-.41-1.2-.67-2.47-.76-3.79h1.51m9.86 12.02c.85.24 1.72.39 2.6.45v1.49c-1.32-.09-2.59-.35-3.8-.75l1.2-1.19M7.5 3H4c-.55 0-1 .45-1 1 0 9.39 7.61 17 17 17 .55 0 1-.45 1-1v-3.49c0-.55-.45-1-1-1-1.24 0-2.45-.2-3.57-.57-.1-.04-.21-.05-.31-.05-.26 0-.51.1-.71.29l-2.2 2.2c-2.83-1.45-5.15-3.76-6.59-6.59l2.2-2.2c.28-.28.36-.67.25-1.02C8.7 6.45 8.5 5.25 8.5 4c0-.55-.45-1-1-1z"></path></svg><p>Contact</p></div></a><a class="navbar-item clickable link" href=#><div><svg class=navbar-icon enable-background="new 0 0 24 24" fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><g><rect fill=none height=24 width=24></rect></g><g><path d="M12,2C6.48,2,2,6.48,2,12c0,5.52,4.48,10,10,10s10-4.48,10-10C22,6.48,17.52,2,12,2z M19.46,9.12l-2.78,1.15 c-0.51-1.36-1.58-2.44-2.95-2.94l1.15-2.78C16.98,5.35,18.65,7.02,19.46,9.12z M12,15c-1.66,0-3-1.34-3-3s1.34-3,3-3s3,1.34,3,3 S13.66,15,12,15z M9.13,4.54l1.17,2.78c-1.38,0.5-2.47,1.59-2.98,2.97L4.54,9.13C5.35,7.02,7.02,5.35,9.13,4.54z M4.54,14.87 l2.78-1.15c0.51,1.38,1.59,2.46,2.97,2.96l-1.17,2.78C7.02,18.65,5.35,16.98,4.54,14.87z M14.88,19.46l-1.15-2.78 c1.37-0.51,2.45-1.59,2.95-2.97l2.78,1.17C18.65,16.98,16.98,18.65,14.88,19.46z"></path></g></svg><p>Support</p></div></a><a class="navbar-item clickable link" href=#><div><svg class=navbar-icon fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0V0z" fill=none></path><path d="M20 6h-2.18c.11-.31.18-.65.18-1 0-1.66-1.34-3-3-3-1.05 0-1.96.54-2.5 1.35l-.5.67-.5-.68C10.96 2.54 10.05 2 9 2 7.34 2 6 3.34 6 5c0 .35.07.69.18 1H4c-1.11 0-1.99.89-1.99 2L2 19c0 1.11.89 2 2 2h16c1.11 0 2-.89 2-2V8c0-1.11-.89-2-2-2zm-5-2c.55 0 1 .45 1 1s-.45 1-1 1-1-.45-1-1 .45-1 1-1zM9 4c.55 0 1 .45 1 1s-.45 1-1 1-1-.45-1-1 .45-1 1-1zm11 15H4v-2h16v2zm0-5H4V8h5.08L7 10.83 8.62 12 12 7.4l3.38 4.6L17 10.83 14.92 8H20v6z"></path></svg><p>Log in</p></div></a></div></div></body></html><div class=docs-container><html><body><div class=sidebar-container><div class=sidebar><a class="sidebar-item clickable" href=https://inobulles.github.io/docs><div><svg fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0V0z" fill=none></path><path d="M12.36 6l.4 2H18v6h-3.36l-.4-2H7V6h5.36M14 4H5v17h2v-7h5.6l.4 2h7V6h-5.6L14 4z"></path></svg><p>Introduction</p></div></a><a class="sidebar-item clickable" href=https://inobulles.github.io/docs/struct><div><svg enable-background="new 0 0 24 24" fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><rect fill=none height=24 width=24></rect><path d="M22,11V3h-7v3H9V3H2v8h7V8h2v10h4v3h7v-8h-7v3h-2V8h2v3H22z M7,9H4V5h3V9z M17,15h3v4h-3V15z M17,5h3v4h-3V5z"></path></svg><p>OS Components</p></div></a><a class="sidebar-item clickable" href=https://inobulles.github.io/docs/dev-tools><div><svg enable-background="new 0 0 24 24" fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><g><rect fill=none height=24 width=24></rect></g><g><g><rect height=8.48 transform="matrix(0.7071 -0.7071 0.7071 0.7071 -6.8717 17.6255)" width=3 x=16.34 y=12.87></rect><path d="M17.5,10c1.93,0,3.5-1.57,3.5-3.5c0-0.58-0.16-1.12-0.41-1.6l-2.7,2.7L16.4,6.11l2.7-2.7C18.62,3.16,18.08,3,17.5,3 C15.57,3,14,4.57,14,6.5c0,0.41,0.08,0.8,0.21,1.16l-1.85,1.85l-1.78-1.78l0.71-0.71L9.88,5.61L12,3.49 c-1.17-1.17-3.07-1.17-4.24,0L4.22,7.03l1.41,1.41H2.81L2.1,9.15l3.54,3.54l0.71-0.71V9.15l1.41,1.41l0.71-0.71l1.78,1.78 l-7.41,7.41l2.12,2.12L16.34,9.79C16.7,9.92,17.09,10,17.5,10z"></path></g></g></svg><p>Developer Tools</p></div></a><a class="sidebar-item clickable" href=https://inobulles.github.io/docs/create-app><div><svg enable-background="new 0 0 24 24" fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><g><rect fill=none height=24 width=24 x=0></rect></g><g><g><polygon points="19,9 20.25,6.25 23,5 20.25,3.75 19,1 17.75,3.75 15,5 17.75,6.25"></polygon><polygon points="19,15 17.75,17.75 15,19 17.75,20.25 19,23 20.25,20.25 23,19 20.25,17.75"></polygon><path d="M11.5,9.5L9,4L6.5,9.5L1,12l5.5,2.5L9,20l2.5-5.5L17,12L11.5,9.5z M9.99,12.99L9,15.17l-0.99-2.18L5.83,12l2.18-0.99 L9,8.83l0.99,2.18L12.17,12L9.99,12.99z"></path></g></g></svg><p>Creating an App</p></div></a><a class="sidebar-item clickable" href=https://inobulles.github.io/docs/packaging><div><svg enable-background="new 0 0 24 24" fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><g><rect fill=none height=24 width=24></rect></g><g><g><path d="M20,2H4C3,2,2,2.9,2,4v3.01C2,7.73,2.43,8.35,3,8.7V20c0,1.1,1.1,2,2,2h14c0.9,0,2-0.9,2-2V8.7c0.57-0.35,1-0.97,1-1.69V4 C22,2.9,21,2,20,2z M19,20H5V9h14V20z M20,7H4V4h16V7z"></path><rect height=2 width=6 x=9 y=12></rect></g></g></svg><p>Packaging Apps</p></div></a><a class="sidebar-item clickable" href=https://inobulles.github.io/docs/publishing><div><svg fill=#000000 height=24px viewbox="0 0 24 24" width=24px xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0V0z" fill=none></path><path d="M13 3v1h-2V3h2m-1 7.11l5.38 1.77 2.39.78-1.12 3.97c-.54-.3-.94-.71-1.14-.94L16 13.96l-1.51 1.72c-.34.4-1.28 1.32-2.49 1.32s-2.15-.92-2.49-1.32L8 13.96l-1.51 1.72c-.2.23-.6.63-1.14.93l-1.13-3.96 2.4-.79L12 10.11M15 1H9v3H6c-1.1 0-2 .9-2 2v4.62l-1.29.42c-.26.08-.48.26-.6.5s-.15.52-.06.78L3.95 19H4c1.6 0 3.02-.88 4-2 .98 1.12 2.4 2 4 2s3.02-.88 4-2c.98 1.12 2.4 2 4 2h.05l1.89-6.68c.08-.26.06-.54-.06-.78s-.34-.42-.6-.5L20 10.62V6c0-1.1-.9-2-2-2h-3V1zM6 9.97V6h12v3.97L12 8 6 9.97zm10 9.71c-1.22.85-2.61 1.28-4 1.28s-2.78-.43-4-1.28C6.78 20.53 5.39 21 4 21H2v2h2c1.38 0 2.74-.35 4-.99 1.26.64 2.63.97 4 .97s2.74-.32 4-.97c1.26.65 2.62.99 4 .99h2v-2h-2c-1.39 0-2.78-.47-4-1.32z"></path></svg><p>Publishing Apps</p></div></a></div></div></body></html><div class=docs><h1>netmap(4)</h1><table class=head><tr><td class=head-ltitle>NETMAP(4)</td><td class=head-vol>FreeBSD Kernel Interfaces Manual</td><td class=head-rtitle>NETMAP(4)</td></tr></table><div class=manual-text><section class=Sh><h2 class=Sh id=NAME><a class=permalink href=#NAME>NAME</a></h2><code class=Nm>netmap</code> — <div class=Nd>a framework for fast packet I/O</div></section><section class=Sh><h2 class=Sh id=SYNOPSIS><a class=permalink href=#SYNOPSIS>SYNOPSIS</a></h2><code class=Cd>device netmap</code></section><section class=Sh><h2 class=Sh id=DESCRIPTION><a class=permalink href=#DESCRIPTION>DESCRIPTION</a></h2><code class=Nm>netmap</code> is a framework for extremely fast and efficient packet I/O for userspace and kernel clients, and for Virtual Machines. It runs on <span class=Ux>FreeBSD</span>, Linux and some versions of Windows, and supports a variety of <code class=Nm>netmap ports</code>, including <dl class=Bl-tag><dt><code class=Nm>physical NIC ports</code></dt><dd>to access individual queues of network interfaces;</dd><dt><code class=Nm>host ports</code></dt><dd>to inject packets into the host stack;</dd><dt><code class=Nm>VALE ports</code></dt><dd>implementing a very fast and modular in-kernel software switch/dataplane;</dd><dt><code class=Nm>netmap pipes</code></dt><dd>a shared memory packet transport channel;</dd><dt><code class=Nm>netmap monitors</code></dt><dd>a mechanism similar to <a class=Xr href=bpf.4.html>bpf(4)</a> to capture traffic</dd></dl><p class=Pp>All these <code class=Nm>netmap ports</code> are accessed interchangeably with the same API, and are at least one order of magnitude faster than standard OS mechanisms (sockets, bpf, tun/tap interfaces, native switches, pipes). With suitably fast hardware (NICs, PCIe buses, CPUs), packet I/O using <code class=Nm>netmap</code> on supported NICs reaches 14.88 million packets per second (Mpps) with much less than one core on 10 Gbit/s NICs; 35-40 Mpps on 40 Gbit/s NICs (limited by the hardware); about 20 Mpps per core for VALE ports; and over 100 Mpps for <code class=Nm>netmap pipes</code>. NICs without native <code class=Nm>netmap</code> support can still use the API in emulated mode, which uses unmodified device drivers and is 3-5 times faster than <a class=Xr href=bpf.4.html>bpf(4)</a> or raw sockets.</p><p class=Pp>Userspace clients can dynamically switch NICs into <code class=Nm>netmap</code> mode and send and receive raw packets through memory mapped buffers. Similarly, <code class=Nm>VALE</code> switch instances and ports, <code class=Nm>netmap pipes</code> and <code class=Nm>netmap monitors</code> can be created dynamically, providing high speed packet I/O between processes, virtual machines, NICs and the host stack.</p><p class=Pp><code class=Nm>netmap</code> supports both non-blocking I/O through <a class=Xr href=ioctl.2.html>ioctl(2)</a>, synchronization and blocking I/O through a file descriptor and standard OS mechanisms such as <a class=Xr href=select.2.html>select(2)</a>, <a class=Xr href=poll.2.html>poll(2)</a>, <a class=Xr href=kqueue.2.html>kqueue(2)</a> and <a class=Xr href=epoll.7.html>epoll(7)</a>. All types of <code class=Nm>netmap ports</code> and the <code class=Nm>VALE switch</code> are implemented by a single kernel module, which also emulates the <code class=Nm>netmap</code> API over standard drivers. For best performance, <code class=Nm>netmap</code> requires native support in device drivers. A list of such devices is at the end of this document.</p><p class=Pp>In the rest of this (long) manual page we document various aspects of the <code class=Nm>netmap</code> and <code class=Nm>VALE</code> architecture, features and usage.</p></section><section class=Sh><h2 class=Sh id=ARCHITECTURE><a class=permalink href=#ARCHITECTURE>ARCHITECTURE</a></h2><code class=Nm>netmap</code> supports raw packet I/O through a <i class=Em>port</i>, which can be connected to a physical interface (<i class=Em>NIC</i>), to the host stack, or to a <code class=Nm>VALE</code> switch. Ports use preallocated circular queues of buffers (<i class=Em>rings</i>) residing in an mmapped region. There is one ring for each transmit/receive queue of a NIC or virtual port. An additional ring pair connects to the host stack. <p class=Pp>After binding a file descriptor to a port, a <code class=Nm>netmap</code> client can send or receive packets in batches through the rings, and possibly implement zero-copy forwarding between ports.</p><p class=Pp>All NICs operating in <code class=Nm>netmap</code> mode use the same memory region, accessible to all processes who own <span class=Pa>/dev/netmap</span> file descriptors bound to NICs. Independent <code class=Nm>VALE</code> and <code class=Nm>netmap pipe</code> ports by default use separate memory regions, but can be independently configured to share memory.</p></section><section class=Sh><h2 class=Sh id=ENTERING_AND_EXITING_NETMAP_MODE><a class=permalink href=#ENTERING_AND_EXITING_NETMAP_MODE>ENTERING AND EXITING NETMAP MODE</a></h2> The following section describes the system calls to create and control <code class=Nm>netmap</code> ports (including <code class=Nm>VALE</code> and <code class=Nm>netmap pipe</code> ports). Simpler, higher level functions are described in the <a class=Sx href=#LIBRARIES>LIBRARIES</a> section. <p class=Pp>Ports and rings are created and controlled through a file descriptor, created by opening a special device</p><div class="Bd Bd-indent"><code class=Li>fd = open("/dev/netmap");</code></div> and then bound to a specific port with an <div class="Bd Bd-indent"><code class=Li>ioctl(fd, NIOCREGIF, (struct nmreq *)arg);</code></div><p class=Pp><code class=Nm>netmap</code> has multiple modes of operation controlled by the <var class=Vt>struct nmreq</var> argument. <var class=Va>arg.nr_name</var> specifies the netmap port name, as follows:</p><dl class=Bl-tag><dt><a class=permalink href="#OS_network_interface_name_(e.g.,_'em0',_'eth1',_..."><code class=Dv id="OS_network_interface_name_(e.g.,_'em0',_'eth1',_...">OS network interface name (e.g., 'em0', 'eth1', ...</code></a>)</dt><dd>the data path of the NIC is disconnected from the host stack, and the file descriptor is bound to the NIC (one or all queues), or to the host stack;</dd><dt><a class=permalink href=#valeSSS:PPP><code class=Dv id=valeSSS:PPP>valeSSS:PPP</code></a></dt><dd>the file descriptor is bound to port PPP of VALE switch SSS. Switch instances and ports are dynamically created if necessary. <p class=Pp>Both SSS and PPP have the form [0-9a-zA-Z_]+ , the string cannot exceed IFNAMSIZ characters, and PPP cannot be the name of any existing OS network interface.</p></dd></dl><p class=Pp>On return, <var class=Va>arg</var> indicates the size of the shared memory region, and the number, size and location of all the <code class=Nm>netmap</code> data structures, which can be accessed by mmapping the memory</p><div class="Bd Bd-indent"><code class=Li>char *mem = mmap(0, arg.nr_memsize, fd);</code></div><p class=Pp>Non-blocking I/O is done with special <a class=Xr href=ioctl.2.html>ioctl(2)</a><a class=Xr href=select.2.html>select(2)</a> and <a class=Xr href=poll.2.html>poll(2)</a> on the file descriptor permit blocking I/O.</p><p class=Pp>While a NIC is in <code class=Nm>netmap</code> mode, the OS will still believe the interface is up and running. OS-generated packets for that NIC end up into a <code class=Nm>netmap</code> ring, and another ring is used to send packets into the OS network stack. A <a class=Xr href=close.2.html>close(2)</a> on the file descriptor removes the binding, and returns the NIC to normal mode (reconnecting the data path to the host stack), or destroys the virtual port.</p></section><section class=Sh><h2 class=Sh id=DATA_STRUCTURES><a class=permalink href=#DATA_STRUCTURES>DATA STRUCTURES</a></h2> The data structures in the mmapped memory region are detailed in <code class=In>&lt;<a class=In href=../src/sys/net/netmap.h.html>sys/net/netmap.h</a>&gt;</code>, which is the ultimate reference for the <code class=Nm>netmap</code> API. The main structures and fields are indicated below: <dl class=Bl-tag><dt><a class=permalink href=#struct_netmap_if_(one_per_interface><code class=Dv id=struct_netmap_if_(one_per_interface>struct netmap_if (one per interface</code></a>)</dt><dd><div class="Bd Pp"><pre>
struct netmap_if {
    ...
    const uint32_t   ni_flags;      /* properties              */
    ...
    const uint32_t   ni_tx_rings;   /* NIC tx rings            */
    const uint32_t   ni_rx_rings;   /* NIC rx rings            */
    uint32_t         ni_bufs_head;  /* head of extra bufs list */
    ...
};
    </pre></div><p class=Pp>Indicates the number of available rings (<span class=Pa>struct netmap_rings</span>) and their position in the mmapped region. The number of tx and rx rings (<span class=Pa>ni_tx_rings</span>, <span class=Pa>ni_rx_rings</span>) normally depends on the hardware. NICs also have an extra tx/rx ring pair connected to the host stack. <i class=Em>NIOCREGIF</i> can also request additional unbound buffers in the same memory space, to be used as temporary storage for packets. The number of extra buffers is specified in the <var class=Va>arg.nr_arg3</var> field. On success, the kernel writes back to <var class=Va>arg.nr_arg3</var> the number of extra buffers actually allocated (they may be less than the amount requested if the memory space ran out of buffers). <span class=Pa>ni_bufs_head</span> contains the index of the first of these extra buffers, which are connected in a list (the first uint32_t of each buffer being the index of the next buffer in the list). A <code class=Dv>0</code> indicates the end of the list. The application is free to modify this list and use the buffers (i.e., binding them to the slots of a netmap ring). When closing the netmap file descriptor, the kernel frees the buffers contained in the list pointed by <span class=Pa>ni_bufs_head</span> , irrespectively of the buffers originally provided by the kernel on <i class=Em>NIOCREGIF</i>.</p></dd><dt><a class=permalink href=#struct_netmap_ring_(one_per_ring><code class=Dv id=struct_netmap_ring_(one_per_ring>struct netmap_ring (one per ring</code></a>)</dt><dd><div class="Bd Pp"><pre>
struct netmap_ring {
    ...
    const uint32_t num_slots;   /* slots in each ring            */
    const uint32_t nr_buf_size; /* size of each buffer           */
    ...
    uint32_t       head;        /* (u) first buf owned by user   */
    uint32_t       cur;         /* (u) wakeup position           */
    const uint32_t tail;        /* (k) first buf owned by kernel */
    ...
    uint32_t       flags;
    struct timeval ts;          /* (k) time of last rxsync()     */
    ...
    struct netmap_slot slot[0]; /* array of slots                */
}
    </pre></div><p class=Pp>Implements transmit and receive rings, with read/write pointers, metadata and an array of <i class=Em>slots</i> describing the buffers.</p></dd><dt><a class=permalink href=#struct_netmap_slot_(one_per_buffer><code class=Dv id=struct_netmap_slot_(one_per_buffer>struct netmap_slot (one per buffer</code></a>)</dt><dd><div class="Bd Pp"><pre>
struct netmap_slot {
    uint32_t buf_idx;           /* buffer index                 */
    uint16_t len;               /* packet length                */
    uint16_t flags;             /* buf changed, etc.            */
    uint64_t ptr;               /* address for indirect buffers */
};
    </pre></div><p class=Pp>Describes a packet buffer, which normally is identified by an index and resides in the mmapped region.</p></dd><dt><a class=permalink href=#packet_buffers><code class=Dv id=packet_buffers>packet buffers</code></a></dt><dd>Fixed size (normally 2 KB) packet buffers allocated by the kernel.</dd></dl><p class=Pp>The offset of the <span class=Pa>struct netmap_if</span> in the mmapped region is indicated by the <span class=Pa>nr_offset</span> field in the structure returned by <code class=Dv>NIOCREGIF</code>. From there, all other objects are reachable through relative references (offsets or indexes). Macros and functions in <code class=In>&lt;<a class=In href=../src/net/netmap_user.h.html>net/netmap_user.h</a>&gt;</code> help converting them into actual pointers:</p><p class=Pp></p><div class="Bd Bd-indent"><code class=Li>struct netmap_if *nifp = NETMAP_IF(mem, arg.nr_offset);</code></div><div class="Bd Bd-indent"><code class=Li>struct netmap_ring *txr = NETMAP_TXRING(nifp, ring_index);</code></div><div class="Bd Bd-indent"><code class=Li>struct netmap_ring *rxr = NETMAP_RXRING(nifp, ring_index);</code></div><p class=Pp></p><div class="Bd Bd-indent"><code class=Li>char *buf = NETMAP_BUF(ring, buffer_index);</code></div></section><section class=Sh><h2 class=Sh id=RINGS,_BUFFERS_AND_DATA_I/O><a class=permalink href=#RINGS,_BUFFERS_AND_DATA_I/O>RINGS, BUFFERS AND DATA I/O</a></h2><var class=Va>Rings</var> are circular queues of packets with three indexes/pointers (<var class=Va>head</var>, <var class=Va>cur</var>, <var class=Va>tail</var>); one slot is always kept empty. The ring size (<var class=Va>num_slots</var>) should not be assumed to be a power of two. <p class=Pp><var class=Va>head</var> is the first slot available to userspace;</p><p class=Pp><var class=Va>cur</var> is the wakeup point: select/poll will unblock when <var class=Va>tail</var> passes <var class=Va>cur</var>;</p><p class=Pp><var class=Va>tail</var> is the first slot reserved to the kernel.</p><p class=Pp>Slot indexes <i class=Em>must</i> only move forward; for convenience, the function</p><div class="Bd Bd-indent"><code class=Li>nm_ring_next(ring, index)</code></div> returns the next index modulo the ring size. <p class=Pp><var class=Va>head</var> and <var class=Va>cur</var> are only modified by the user program; <var class=Va>tail</var> is only modified by the kernel. The kernel only reads/writes the <var class=Vt>struct netmap_ring</var> slots and buffers during the execution of a netmap-related system call. The only exception are slots (and buffers) in the range <var class=Va>tail </var>... <var class=Va>head-1</var>, that are explicitly assigned to the kernel.</p><section class=Ss><h2 class=Ss id=TRANSMIT_RINGS><a class=permalink href=#TRANSMIT_RINGS>TRANSMIT RINGS</a></h2> On transmit rings, after a <code class=Nm>netmap</code> system call, slots in the range <var class=Va>head </var>... <var class=Va>tail-1</var> are available for transmission. User code should fill the slots sequentially and advance <var class=Va>head</var> and <var class=Va>cur</var> past slots ready to transmit. <var class=Va>cur</var> may be moved further ahead if the user code needs more slots before further transmissions (see <a class=Sx href=#SCATTER_GATHER_I/O>SCATTER GATHER I/O</a>). <p class=Pp>At the next NIOCTXSYNC/select()/poll(), slots up to <var class=Va>head-1</var> are pushed to the port, and <var class=Va>tail</var> may advance if further slots have become available. Below is an example of the evolution of a TX ring:</p><div class="Bd Pp"><pre>
    after the syscall, slots between cur and tail are (a)vailable
              head=cur   tail
               |          |
               v          v
     TX  [.....aaaaaaaaaaa.............]

    user creates new packets to (T)ransmit
                head=cur tail
                    |     |
                    v     v
     TX  [.....TTTTTaaaaaa.............]

    NIOCTXSYNC/poll()/select() sends packets and reports new slots
                head=cur      tail
                    |          |
                    v          v
     TX  [..........aaaaaaaaaaa........]
</pre></div><p class=Pp><code class=Fn>select</code>() and <code class=Fn>poll</code>() will block if there is no space in the ring, i.e.,</p><div class="Bd Bd-indent"><code class=Li>ring-&gt;cur == ring-&gt;tail</code></div> and return when new slots have become available. <p class=Pp>High speed applications may want to amortize the cost of system calls by preparing as many packets as possible before issuing them.</p><p class=Pp>A transmit ring with pending transmissions has</p><div class="Bd Bd-indent"><code class=Li>ring-&gt;head != ring-&gt;tail + 1 (modulo the ring size).</code></div> The function <var class=Va>int nm_tx_pending(ring)</var> implements this test. </section><section class=Ss><h2 class=Ss id=RECEIVE_RINGS><a class=permalink href=#RECEIVE_RINGS>RECEIVE RINGS</a></h2> On receive rings, after a <code class=Nm>netmap</code> system call, the slots in the range <var class=Va>head</var>... <var class=Va>tail-1</var> contain received packets. User code should process them and advance <var class=Va>head</var> and <var class=Va>cur</var> past slots it wants to return to the kernel. <var class=Va>cur</var> may be moved further ahead if the user code wants to wait for more packets without returning all the previous slots to the kernel. <p class=Pp>At the next NIOCRXSYNC/select()/poll(), slots up to <var class=Va>head-1</var> are returned to the kernel for further receives, and <var class=Va>tail</var> may advance to report new incoming packets.</p><p class=Pp>Below is an example of the evolution of an RX ring:</p><div class="Bd Pp"><pre>
    after the syscall, there are some (h)eld and some (R)eceived slots
           head  cur     tail
            |     |       |
            v     v       v
     RX  [..hhhhhhRRRRRRRR..........]

    user advances head and cur, releasing some slots and holding others
               head cur  tail
                 |  |     |
                 v  v     v
     RX  [..*****hhhRRRRRR...........]

    NICRXSYNC/poll()/select() recovers slots and reports new packets
               head cur        tail
                 |  |           |
                 v  v           v
     RX  [.......hhhRRRRRRRRRRRR....]
</pre></div></section></section><section class=Sh><h2 class=Sh id=SLOTS_AND_PACKET_BUFFERS><a class=permalink href=#SLOTS_AND_PACKET_BUFFERS>SLOTS AND PACKET BUFFERS</a></h2> Normally, packets should be stored in the netmap-allocated buffers assigned to slots when ports are bound to a file descriptor. One packet is fully contained in a single buffer. <p class=Pp>The following flags affect slot and buffer processing:</p><dl class=Bl-tag><dt>NS_BUF_CHANGED</dt><dd><i class=Em>must</i> be used when the <var class=Va>buf_idx</var> in the slot is changed. This can be used to implement zero-copy forwarding, see <a class=Sx href=#ZERO_COPY_FORWARDING>ZERO-COPY FORWARDING</a>.</dd><dt>NS_REPORT</dt><dd>reports when this buffer has been transmitted. Normally, <code class=Nm>netmap</code> notifies transmit completions in batches, hence signals can be delayed indefinitely. This flag helps detect when packets have been sent and a file descriptor can be closed.</dd><dt>NS_FORWARD</dt><dd>When a ring is in 'transparent' mode, packets marked with this flag by the user application are forwarded to the other endpoint at the next system call, thus restoring (in a selective way) the connection between a NIC and the host stack.</dd><dt>NS_NO_LEARN</dt><dd>tells the forwarding code that the source MAC address for this packet must not be used in the learning bridge code.</dd><dt>NS_INDIRECT</dt><dd>indicates that the packet's payload is in a user-supplied buffer whose user virtual address is in the 'ptr' field of the slot. The size can reach 65535 bytes. <p class=Pp>This is only supported on the transmit ring of <code class=Nm>VALE</code> ports, and it helps reducing data copies in the interconnection of virtual machines.</p></dd><dt>NS_MOREFRAG</dt><dd>indicates that the packet continues with subsequent buffers; the last buffer in a packet must have the flag clear.</dd></dl></section><section class=Sh><h2 class=Sh id=SCATTER_GATHER_I/O><a class=permalink href=#SCATTER_GATHER_I/O>SCATTER GATHER I/O</a></h2> Packets can span multiple slots if the <var class=Va>NS_MOREFRAG</var> flag is set in all but the last slot. The maximum length of a chain is 64 buffers. This is normally used with <code class=Nm>VALE</code> ports when connecting virtual machines, as they generate large TSO segments that are not split unless they reach a physical device. <p class=Pp>NOTE: The length field always refers to the individual fragment; there is no place with the total length of a packet.</p><p class=Pp>On receive rings the macro <var class=Va>NS_RFRAGS(slot)</var> indicates the remaining number of slots for this packet, including the current one. Slots with a value greater than 1 also have NS_MOREFRAG set.</p></section><section class=Sh><h2 class=Sh id=IOCTLS><a class=permalink href=#IOCTLS>IOCTLS</a></h2><code class=Nm>netmap</code> uses two ioctls (NIOCTXSYNC, NIOCRXSYNC) for non-blocking I/O. They take no argument. Two more ioctls (NIOCGINFO, NIOCREGIF) are used to query and configure ports, with the following argument: <div class="Bd Pp"><pre>
struct nmreq {
    char      nr_name[IFNAMSIZ]; /* (i) port name                  */
    uint32_t  nr_version;        /* (i) API version                */
    uint32_t  nr_offset;         /* (o) nifp offset in mmap region */
    uint32_t  nr_memsize;        /* (o) size of the mmap region    */
    uint32_t  nr_tx_slots;       /* (i/o) slots in tx rings        */
    uint32_t  nr_rx_slots;       /* (i/o) slots in rx rings        */
    uint16_t  nr_tx_rings;       /* (i/o) number of tx rings       */
    uint16_t  nr_rx_rings;       /* (i/o) number of rx rings       */
    uint16_t  nr_ringid;         /* (i/o) ring(s) we care about    */
    uint16_t  nr_cmd;            /* (i) special command            */
    uint16_t  nr_arg1;           /* (i/o) extra arguments          */
    uint16_t  nr_arg2;           /* (i/o) extra arguments          */
    uint32_t  nr_arg3;           /* (i/o) extra arguments          */
    uint32_t  nr_flags           /* (i/o) open mode                */
    ...
};
</pre></div><p class=Pp>A file descriptor obtained through <span class=Pa>/dev/netmap</span> also supports the ioctl supported by network devices, see <a class=Xr href=netintro.4.html>netintro(4)</a>.</p><dl class=Bl-tag><dt><a class=permalink href=#NIOCGINFO><code class=Dv id=NIOCGINFO>NIOCGINFO</code></a></dt><dd>returns EINVAL if the named port does not support netmap. Otherwise, it returns 0 and (advisory) information about the port. Note that all the information below can change before the interface is actually put in netmap mode. <dl class=Bl-tag><dt><span class=Pa>nr_memsize</span></dt><dd>indicates the size of the <code class=Nm>netmap</code> memory region. NICs in <code class=Nm>netmap</code> mode all share the same memory region, whereas <code class=Nm>VALE</code> ports have independent regions for each port.</dd><dt><span class=Pa>nr_tx_slots</span>, <span class=Pa>nr_rx_slots</span></dt><dd>indicate the size of transmit and receive rings.</dd><dt><span class=Pa>nr_tx_rings</span>, <span class=Pa>nr_rx_rings</span></dt><dd>indicate the number of transmit and receive rings. Both ring number and sizes may be configured at runtime using interface-specific functions (e.g., <a class=Xr href=ethtool.8.html>ethtool(8)</a> ).</dd></dl></dd><dt><a class=permalink href=#NIOCREGIF><code class=Dv id=NIOCREGIF>NIOCREGIF</code></a></dt><dd>binds the port named in <var class=Va>nr_name</var> to the file descriptor. For a physical device this also switches it into <code class=Nm>netmap</code> mode, disconnecting it from the host stack. Multiple file descriptors can be bound to the same port, with proper synchronization left to the user. <p class=Pp>The recommended way to bind a file descriptor to a port is to use function <var class=Va>nm_open(..)</var> (see <a class=Sx href=#LIBRARIES>LIBRARIES</a>) which parses names to access specific port types and enable features. In the following we document the main features.</p><p class=Pp><code class=Dv>NIOCREGIF can also bind a file descriptor to one endpoint of a</code><i class=Em>netmap pipe</i>, consisting of two netmap ports with a crossover connection. A netmap pipe share the same memory space of the parent port, and is meant to enable configuration where a master process acts as a dispatcher towards slave processes.</p><p class=Pp>To enable this function, the <span class=Pa>nr_arg1</span> field of the structure can be used as a hint to the kernel to indicate how many pipes we expect to use, and reserve extra space in the memory region.</p><p class=Pp>On return, it gives the same info as NIOCGINFO, with <span class=Pa>nr_ringid</span> and <span class=Pa>nr_flags</span> indicating the identity of the rings controlled through the file descriptor.</p><p class=Pp><var class=Va>nr_flags</var><var class=Va>nr_ringid</var> selects which rings are controlled through this file descriptor. Possible values of <span class=Pa>nr_flags</span> are indicated below, together with the naming schemes that application libraries (such as the <code class=Nm>nm_open</code> indicated below) can use to indicate the specific set of rings. In the example below, "netmap:foo" is any valid netmap port name.</p><dl class=Bl-tag><dt>NR_REG_ALL_NIC netmap:foo</dt><dd>(default) all hardware ring pairs</dd><dt>NR_REG_SW netmap:foo^</dt><dd>the ``host rings'', connecting to the host stack.</dd><dt>NR_REG_NIC_SW netmap:foo*</dt><dd>all hardware rings and the host rings</dd><dt>NR_REG_ONE_NIC netmap:foo-i</dt><dd>only the i-th hardware ring pair, where the number is in <span class=Pa>nr_ringid</span>;</dd><dt>NR_REG_PIPE_MASTER netmap:foo{i</dt><dd>the master side of the netmap pipe whose identifier (i) is in <span class=Pa>nr_ringid</span>;</dd><dt>NR_REG_PIPE_SLAVE netmap:foo}i</dt><dd>the slave side of the netmap pipe whose identifier (i) is in <span class=Pa>nr_ringid</span>. <p class=Pp>The identifier of a pipe must be thought as part of the pipe name, and does not need to be sequential. On return the pipe will only have a single ring pair with index 0, irrespective of the value of <var class=Va>i</var>.</p></dd></dl><p class=Pp>By default, a <a class=Xr href=poll.2.html>poll(2)</a> or <a class=Xr href=select.2.html>select(2)</a> call pushes out any pending packets on the transmit ring, even if no write events are specified. The feature can be disabled by or-ing <var class=Va>NETMAP_NO_TX_POLL</var> to the value written to <var class=Va>nr_ringid</var>. When this feature is used, packets are transmitted only on <var class=Va>ioctl(NIOCTXSYNC)</var> or <var class=Va>select() /</var><var class=Va>poll()</var> are called with a write event (POLLOUT/wfdset) or a full ring.</p><p class=Pp>When registering a virtual interface that is dynamically created to a <code class=Nm>VALE</code> switch, we can specify the desired number of rings (1 by default, and currently up to 16) on it using nr_tx_rings and nr_rx_rings fields.</p></dd><dt><a class=permalink href=#NIOCTXSYNC><code class=Dv id=NIOCTXSYNC>NIOCTXSYNC</code></a></dt><dd>tells the hardware of new packets to transmit, and updates the number of slots available for transmission.</dd><dt><a class=permalink href=#NIOCRXSYNC><code class=Dv id=NIOCRXSYNC>NIOCRXSYNC</code></a></dt><dd>tells the hardware of consumed packets, and asks for newly available packets.</dd></dl></section><section class=Sh><h2 class=Sh id=SELECT,_POLL,_EPOLL,_KQUEUE><a class=permalink href=#SELECT,_POLL,_EPOLL,_KQUEUE>SELECT, POLL, EPOLL, KQUEUE</a></h2><a class=Xr href=select.2.html>select(2)</a> and <a class=Xr href=poll.2.html>poll(2)</a> on a <code class=Nm>netmap</code> file descriptor process rings as indicated in <a class=Sx href=#TRANSMIT_RINGS>TRANSMIT RINGS</a> and <a class=Sx href=#RECEIVE_RINGS>RECEIVE RINGS</a>, respectively when write (POLLOUT) and read (POLLIN) events are requested. Both block if no slots are available in the ring (<var class=Va>ring-&gt;cur == ring-&gt;tail</var>). Depending on the platform, <a class=Xr href=epoll.7.html>epoll(7)</a> and <a class=Xr href=kqueue.2.html>kqueue(2)</a> are supported too. <p class=Pp>Packets in transmit rings are normally pushed out (and buffers reclaimed) even without requesting write events. Passing the <code class=Dv>NETMAP_NO_TX_POLL</code> flag to <i class=Em>NIOCREGIF</i> disables this feature. By default, receive rings are processed only if read events are requested. Passing the <code class=Dv>NETMAP_DO_RX_POLL</code> flag to <i class=Em>NIOCREGIF updates receive rings even without read events.</i> Note that on <a class=Xr href=epoll.7.html>epoll(7)</a> and <a class=Xr href=kqueue.2.html>kqueue(2)</a>, <code class=Dv>NETMAP_NO_TX_POLL</code> and <code class=Dv>NETMAP_DO_RX_POLL</code> only have an effect when some event is posted for the file descriptor.</p></section><section class=Sh><h2 class=Sh id=LIBRARIES><a class=permalink href=#LIBRARIES>LIBRARIES</a></h2> The <code class=Nm>netmap</code> API is supposed to be used directly, both because of its simplicity and for efficient integration with applications. <p class=Pp>For convenience, the <code class=In>&lt;<a class=In href=../src/net/netmap_user.h.html>net/netmap_user.h</a>&gt;</code> header provides a few macros and functions to ease creating a file descriptor and doing I/O with a <code class=Nm>netmap</code> port. These are loosely modeled after the <a class=Xr href=pcap.3.html>pcap(3)</a> API, to ease porting of libpcap-based applications to <code class=Nm>netmap</code>. To use these extra functions, programs should</p><div class="Bd Bd-indent"><code class=Li>#define NETMAP_WITH_LIBS</code></div> before <div class="Bd Bd-indent"><code class=Li>#include &lt;net/netmap_user.h&gt;</code></div><p class=Pp>The following functions are available:</p><dl class=Bl-tag><dt><var class=Va>struct nm_desc * nm_open(const char *ifname, const struct nmreq *req, uint64_t flags, const struct nm_desc *arg</var>)</dt><dd>similar to <a class=Xr href=pcap_open_live.3.html>pcap_open_live(3)</a>, binds a file descriptor to a port. <dl class=Bl-tag><dt><var class=Va>ifname</var></dt><dd>is a port name, in the form "netmap:PPP" for a NIC and "valeSSS:PPP" for a <code class=Nm>VALE</code> port.</dd><dt><var class=Va>req</var></dt><dd>provides the initial values for the argument to the NIOCREGIF ioctl. The nm_flags and nm_ringid values are overwritten by parsing ifname and flags, and other fields can be overridden through the other two arguments.</dd><dt><var class=Va>arg</var></dt><dd>points to a struct nm_desc containing arguments (e.g., from a previously open file descriptor) that should override the defaults. The fields are used as described below</dd><dt><var class=Va>flags</var></dt><dd>can be set to a combination of the following flags: <var class=Va>NETMAP_NO_TX_POLL</var>, <var class=Va>NETMAP_DO_RX_POLL</var> (copied into nr_ringid); <var class=Va>NM_OPEN_NO_MMAP</var> (if arg points to the same memory region, avoids the mmap and uses the values from it); <var class=Va>NM_OPEN_IFNAME</var> (ignores ifname and uses the values in arg); <var class=Va>NM_OPEN_ARG1</var>, <var class=Va>NM_OPEN_ARG2</var>, <var class=Va>NM_OPEN_ARG3</var> (uses the fields from arg); <var class=Va>NM_OPEN_RING_CFG</var> (uses the ring number and sizes from arg).</dd></dl></dd><dt><var class=Va>int nm_close(struct nm_desc *d</var>)</dt><dd>closes the file descriptor, unmaps memory, frees resources.</dd><dt><var class=Va>int nm_inject(struct nm_desc *d, const void *buf, size_t size</var>)</dt><dd>similar to <var class=Va>pcap_inject()</var>, pushes a packet to a ring, returns the size of the packet is successful, or 0 on error;</dd><dt><var class=Va>int nm_dispatch(struct nm_desc *d, int cnt, nm_cb_t cb, u_char *arg</var>)</dt><dd>similar to <var class=Va>pcap_dispatch()</var>, applies a callback to incoming packets</dd><dt><var class=Va>u_char * nm_nextpkt(struct nm_desc *d, struct nm_pkthdr *hdr</var>)</dt><dd>similar to <var class=Va>pcap_next()</var>, fetches the next packet</dd></dl></section><section class=Sh><h2 class=Sh id=SUPPORTED_DEVICES><a class=permalink href=#SUPPORTED_DEVICES>SUPPORTED DEVICES</a></h2><code class=Nm>netmap</code> natively supports the following devices: <p class=Pp>On <span class=Ux>FreeBSD</span>: <a class=Xr href=cxgbe.4.html>cxgbe(4)</a>, <a class=Xr href=em.4.html>em(4)</a>, <a class=Xr href=iflib.4.html>iflib(4)</a> (providing <a class=Xr href=igb.4.html>igb(4)</a> and <a class=Xr href=em.4.html>em(4)</a>), <a class=Xr href=ixgbe.4.html>ixgbe(4)</a>, <a class=Xr href=ixl.4.html>ixl(4)</a>, <a class=Xr href=re.4.html>re(4)</a>, <a class=Xr href=vtnet.4.html>vtnet(4)</a>.</p><p class=Pp>On Linux e1000, e1000e, i40e, igb, ixgbe, ixgbevf, r8169, virtio_net, vmxnet3.</p><p class=Pp>NICs without native support can still be used in <code class=Nm>netmap</code> mode through emulation. Performance is inferior to native netmap mode but still significantly higher than various raw socket types (bpf, PF_PACKET, etc.). Note that for slow devices (such as 1 Gbit/s and slower NICs, or several 10 Gbit/s NICs whose hardware is unable to sustain line rate), emulated and native mode will likely have similar or same throughput.</p><p class=Pp>When emulation is in use, packet sniffer programs such as tcpdump could see received packets before they are diverted by netmap. This behaviour is not intentional, being just an artifact of the implementation of emulation. Note that in case the netmap application subsequently moves packets received from the emulated adapter onto the host RX ring, the sniffer will intercept those packets again, since the packets are injected to the host stack as they were received by the network interface.</p><p class=Pp>Emulation is also available for devices with native netmap support, which can be used for testing or performance comparison. The sysctl variable <var class=Va>dev.netmap.admode</var> globally controls how netmap mode is implemented.</p></section><section class=Sh><h2 class=Sh id=SYSCTL_VARIABLES_AND_MODULE_PARAMETERS><a class=permalink href=#SYSCTL_VARIABLES_AND_MODULE_PARAMETERS>SYSCTL VARIABLES AND MODULE PARAMETERS</a></h2> Some aspects of the operation of <code class=Nm>netmap</code> and <code class=Nm>VALE</code> are controlled through sysctl variables on <span class=Ux>FreeBSD</span> (<i class=Em>dev.netmap.*</i>) and module parameters on Linux (<i class=Em>/sys/module/netmap/parameters/*</i>): <dl class=Bl-tag><dt><var class=Va>dev.netmap.admode: 0</var></dt><dd>Controls the use of native or emulated adapter mode. <p class=Pp>0 uses the best available option;</p><p class=Pp>1 forces native mode and fails if not available;</p><p class=Pp>2 forces emulated hence never fails.</p></dd><dt><var class=Va>dev.netmap.generic_rings: 1</var></dt><dd>Number of rings used for emulated netmap mode</dd><dt><var class=Va>dev.netmap.generic_ringsize: 1024</var></dt><dd>Ring size used for emulated netmap mode</dd><dt><var class=Va>dev.netmap.generic_mit: 100000</var></dt><dd>Controls interrupt moderation for emulated mode</dd><dt><var class=Va>dev.netmap.fwd: 0</var></dt><dd>Forces NS_FORWARD mode</dd><dt><var class=Va>dev.netmap.txsync_retry: 2</var></dt><dd>Number of txsync loops in the <code class=Nm>VALE</code> flush function</dd><dt><var class=Va>dev.netmap.no_pendintr: 1</var></dt><dd>Forces recovery of transmit buffers on system calls</dd><dt><var class=Va>dev.netmap.no_timestamp: 0</var></dt><dd>Disables the update of the timestamp in the netmap ring</dd><dt><var class=Va>dev.netmap.verbose: 0</var></dt><dd>Verbose kernel messages</dd><dt><var class=Va>dev.netmap.buf_num: 163840</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.buf_size: 2048</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.ring_num: 200</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.ring_size: 36864</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.if_num: 100</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.if_size: 1024</var></dt><dd>Sizes and number of objects (netmap_if, netmap_ring, buffers) for the global memory region. The only parameter worth modifying is <var class=Va>dev.netmap.buf_num</var> as it impacts the total amount of memory used by netmap.</dd><dt><var class=Va>dev.netmap.buf_curr_num: 0</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.buf_curr_size: 0</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.ring_curr_num: 0</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.ring_curr_size: 0</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.if_curr_num: 0</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.if_curr_size: 0</var></dt><dd>Actual values in use.</dd><dt><var class=Va>dev.netmap.priv_buf_num: 4098</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.priv_buf_size: 2048</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.priv_ring_num: 4</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.priv_ring_size: 20480</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.priv_if_num: 2</var></dt><dd style="width: auto;"> </dd><dt><var class=Va>dev.netmap.priv_if_size: 1024</var></dt><dd>Sizes and number of objects (netmap_if, netmap_ring, buffers) for private memory regions. A separate memory region is used for each <code class=Nm>VALE</code> port and each pair of <code class=Nm>netmap pipes</code>.</dd><dt><var class=Va>dev.netmap.bridge_batch: 1024</var></dt><dd>Batch size used when moving packets across a <code class=Nm>VALE</code> switch. Values above 64 generally guarantee good performance.</dd><dt><var class=Va>dev.netmap.max_bridges: 8</var></dt><dd>Max number of <code class=Nm>VALE</code> switches that can be created. This tunable can be specified at loader time.</dd><dt><var class=Va>dev.netmap.ptnet_vnet_hdr: 1</var></dt><dd>Allow ptnet devices to use virtio-net headers</dd></dl></section><section class=Sh><h2 class=Sh id=SYSTEM_CALLS><a class=permalink href=#SYSTEM_CALLS>SYSTEM CALLS</a></h2><code class=Nm>netmap</code> uses <a class=Xr href=select.2.html>select(2)</a>, <a class=Xr href=poll.2.html>poll(2)</a>, <a class=Xr href=epoll.7.html>epoll(7)</a> and <a class=Xr href=kqueue.2.html>kqueue(2)</a> to wake up processes when significant events occur, and <a class=Xr href=mmap.2.html>mmap(2)</a> to map memory. <a class=Xr href=ioctl.2.html>ioctl(2)</a> is used to configure ports and <code class=Nm>VALE switches</code>. <p class=Pp>Applications may need to create threads and bind them to specific cores to improve performance, using standard OS primitives, see <a class=Xr href=pthread.3.html>pthread(3)</a>. In particular, <a class=Xr href=pthread_setaffinity_np.3.html>pthread_setaffinity_np(3)</a> may be of use.</p></section><section class=Sh><h2 class=Sh id=EXAMPLES><a class=permalink href=#EXAMPLES>EXAMPLES</a></h2><section class=Ss><h2 class=Ss id=TEST_PROGRAMS><a class=permalink href=#TEST_PROGRAMS>TEST PROGRAMS</a></h2><code class=Nm>netmap</code> comes with a few programs that can be used for testing or simple applications. See the <span class=Pa>examples/</span> directory in <code class=Nm>netmap</code> distributions, or <span class=Pa>tools/tools/netmap/</span> directory in <span class=Ux>FreeBSD</span> distributions. <p class=Pp><a class=Xr href=pkt-gen.8.html>pkt-gen(8)</a> is a general purpose traffic source/sink.</p><p class=Pp>As an example</p><div class="Bd Bd-indent"><code class=Li>pkt-gen -i ix0 -f tx -l 60</code></div> can generate an infinite stream of minimum size packets, and <div class="Bd Bd-indent"><code class=Li>pkt-gen -i ix0 -f rx</code></div> is a traffic sink. Both print traffic statistics, to help monitor how the system performs. <p class=Pp><a class=Xr href=pkt-gen.8.html>pkt-gen(8)</a> has many options can be uses to set packet sizes, addresses, rates, and use multiple send/receive threads and cores.</p><p class=Pp><a class=Xr href=bridge.4.html>bridge(4)</a> is another test program which interconnects two <code class=Nm>netmap</code> ports. It can be used for transparent forwarding between interfaces, as in</p><div class="Bd Bd-indent"><code class=Li>bridge -i netmap:ix0 -i netmap:ix1</code></div> or even connect the NIC to the host stack using netmap <div class="Bd Bd-indent"><code class=Li>bridge -i netmap:ix0</code></div></section><section class=Ss><h2 class=Ss id=USING_THE_NATIVE_API><a class=permalink href=#USING_THE_NATIVE_API>USING THE NATIVE API</a></h2> The following code implements a traffic generator: <p class=Pp></p><div class=Bd><pre>
#include &lt;net/netmap_user.h&gt;
...
void sender(void)
{
    struct netmap_if *nifp;
    struct netmap_ring *ring;
    struct nmreq nmr;
    struct pollfd fds;

    fd = open("/dev/netmap", O_RDWR);
    bzero(&amp;nmr, sizeof(nmr));
    strcpy(nmr.nr_name, "ix0");
    nmr.nm_version = NETMAP_API;
    ioctl(fd, NIOCREGIF, &amp;nmr);
    p = mmap(0, nmr.nr_memsize, fd);
    nifp = NETMAP_IF(p, nmr.nr_offset);
    ring = NETMAP_TXRING(nifp, 0);
    fds.fd = fd;
    fds.events = POLLOUT;
    for (;;) {
	poll(&amp;fds, 1, -1);
	while (!nm_ring_empty(ring)) {
	    i = ring-&gt;cur;
	    buf = NETMAP_BUF(ring, ring-&gt;slot[i].buf_index);
	    ... prepare packet in buf ...
	    ring-&gt;slot[i].len = ... packet length ...
	    ring-&gt;head = ring-&gt;cur = nm_ring_next(ring, i);
	}
    }
}
</pre></div></section><section class=Ss><h2 class=Ss id=HELPER_FUNCTIONS><a class=permalink href=#HELPER_FUNCTIONS>HELPER FUNCTIONS</a></h2> A simple receiver can be implemented using the helper functions: <p class=Pp></p><div class=Bd><pre>
#define NETMAP_WITH_LIBS
#include &lt;net/netmap_user.h&gt;
...
void receiver(void)
{
    struct nm_desc *d;
    struct pollfd fds;
    u_char *buf;
    struct nm_pkthdr h;
    ...
    d = nm_open("netmap:ix0", NULL, 0, 0);
    fds.fd = NETMAP_FD(d);
    fds.events = POLLIN;
    for (;;) {
	poll(&amp;fds, 1, -1);
        while ( (buf = nm_nextpkt(d, &amp;h)) )
	    consume_pkt(buf, h.len);
    }
    nm_close(d);
}
</pre></div></section><section class=Ss><h2 class=Ss id=ZERO_COPY_FORWARDING><a class=permalink href=#ZERO_COPY_FORWARDING>ZERO-COPY FORWARDING</a></h2> Since physical interfaces share the same memory region, it is possible to do packet forwarding between ports swapping buffers. The buffer from the transmit ring is used to replenish the receive ring: <p class=Pp></p><div class=Bd><pre>
    uint32_t tmp;
    struct netmap_slot *src, *dst;
    ...
    src = &amp;src_ring-&gt;slot[rxr-&gt;cur];
    dst = &amp;dst_ring-&gt;slot[txr-&gt;cur];
    tmp = dst-&gt;buf_idx;
    dst-&gt;buf_idx = src-&gt;buf_idx;
    dst-&gt;len = src-&gt;len;
    dst-&gt;flags = NS_BUF_CHANGED;
    src-&gt;buf_idx = tmp;
    src-&gt;flags = NS_BUF_CHANGED;
    rxr-&gt;head = rxr-&gt;cur = nm_ring_next(rxr, rxr-&gt;cur);
    txr-&gt;head = txr-&gt;cur = nm_ring_next(txr, txr-&gt;cur);
    ...
</pre></div></section><section class=Ss><h2 class=Ss id=ACCESSING_THE_HOST_STACK><a class=permalink href=#ACCESSING_THE_HOST_STACK>ACCESSING THE HOST STACK</a></h2> The host stack is for all practical purposes just a regular ring pair, which you can access with the netmap API (e.g., with <div class="Bd Bd-indent"><code class=Li>nm_open("netmap:eth0^", ...);</code></div> All packets that the host would send to an interface in <code class=Nm>netmap</code> mode end up into the RX ring, whereas all packets queued to the TX ring are send up to the host stack. </section><section class=Ss><h2 class=Ss id=VALE_SWITCH><a class=permalink href=#VALE_SWITCH>VALE SWITCH</a></h2> A simple way to test the performance of a <code class=Nm>VALE</code> switch is to attach a sender and a receiver to it, e.g., running the following in two different terminals: <div class="Bd Bd-indent"><code class=Li>pkt-gen -i vale1:a -f rx # receiver</code></div><div class="Bd Bd-indent"><code class=Li>pkt-gen -i vale1:b -f tx # sender</code></div> The same example can be used to test netmap pipes, by simply changing port names, e.g., <div class="Bd Bd-indent"><code class=Li>pkt-gen -i vale2:x{3 -f rx # receiver on the master side</code></div><div class="Bd Bd-indent"><code class=Li>pkt-gen -i vale2:x}3 -f tx # sender on the slave side</code></div><p class=Pp>The following command attaches an interface and the host stack to a switch:</p><div class="Bd Bd-indent"><code class=Li>valectl -h vale2:em0</code></div> Other <code class=Nm>netmap</code> clients attached to the same switch can now communicate with the network card or the host. </section></section><section class=Sh><h2 class=Sh id=SEE_ALSO><a class=permalink href=#SEE_ALSO>SEE ALSO</a></h2><a class=Xr href=vale.4.html>vale(4)</a>, <a class=Xr href=bridge.8.html>bridge(8)</a>, <a class=Xr href=valectl.8.html>valectl(8)</a>, <a class=Xr href=lb.8.html>lb(8)</a>, <a class=Xr href=nmreplay.8.html>nmreplay(8)</a>, <a class=Xr href=pkt-gen.8.html>pkt-gen(8)</a><p class=Pp><span class=Pa>http://info.iet.unipi.it/~luigi/netmap/</span></p><p class=Pp>Luigi Rizzo, Revisiting network I/O APIs: the netmap framework, Communications of the ACM, 55 (3), pp.45-51, March 2012</p><p class=Pp>Luigi Rizzo, netmap: a novel framework for fast packet I/O, Usenix ATC'12, June 2012, Boston</p><p class=Pp>Luigi Rizzo, Giuseppe Lettieri, VALE, a switched ethernet for virtual machines, ACM CoNEXT'12, December 2012, Nice</p><p class=Pp>Luigi Rizzo, Giuseppe Lettieri, Vincenzo Maffione, Speeding up packet I/O in virtual machines, ACM/IEEE ANCS'13, October 2013, San Jose</p></section><section class=Sh><h2 class=Sh id=AUTHORS><a class=permalink href=#AUTHORS>AUTHORS</a></h2> The <code class=Nm>netmap</code> framework has been originally designed and implemented at the Universita` di Pisa in 2011 by <span class=An>Luigi Rizzo</span>, and further extended with help from <span class=An>Matteo Landi</span>, <span class=An>Gaetano Catalli</span>, <span class=An>Giuseppe Lettieri</span>, and <span class=An>Vincenzo Maffione</span>. <p class=Pp><code class=Nm>netmap</code> and <code class=Nm>VALE</code> have been funded by the European Commission within FP7 Projects CHANGE (257422) and OPENLAB (287581).</p></section><section class=Sh><h2 class=Sh id=CAVEATS><a class=permalink href=#CAVEATS>CAVEATS</a></h2> No matter how fast the CPU and OS are, achieving line rate on 10G and faster interfaces requires hardware with sufficient performance. Several NICs are unable to sustain line rate with small packet sizes. Insufficient PCIe or memory bandwidth can also cause reduced performance. <p class=Pp>Another frequent reason for low performance is the use of flow control on the link: a slow receiver can limit the transmit speed. Be sure to disable flow control when running high speed experiments.</p><section class=Ss><h2 class=Ss id=SPECIAL_NIC_FEATURES><a class=permalink href=#SPECIAL_NIC_FEATURES>SPECIAL NIC FEATURES</a></h2><code class=Nm>netmap</code> is orthogonal to some NIC features such as multiqueue, schedulers, packet filters. <p class=Pp>Multiple transmit and receive rings are supported natively and can be configured with ordinary OS tools, such as <a class=Xr href=ethtool.8.html>ethtool(8)</a> or device-specific sysctl variables. The same goes for Receive Packet Steering (RPS) and filtering of incoming traffic.</p><p class=Pp><code class=Nm>netmap</code><i class=Em>does not use</i> features such as <i class=Em>checksum offloading</i>, <i class=Em>TCP segmentation offloading</i>, <i class=Em>encryption</i>, <i class=Em>VLAN encapsulation/decapsulation</i>, etc. When using netmap to exchange packets with the host stack, make sure to disable these features.</p></section></section></div><table class=foot><tr><td class=foot-date>March 6, 2022</td><td class=foot-os>FreeBSD 13.1-RELEASE-p2</td></tr></table></div></div><html><body><footer><p>©️ 2023 Inobulles</p></footer></body></html></body></html>